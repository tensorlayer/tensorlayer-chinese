<!--<div align="center">
	<div class="TensorFlow">
  <img src="https://www.tensorflow.org/images/tf_logo_transp.png" style=": left; margin-left: 5px; margin-bottom: 5px;"><br><br>
   </div>
   <div class="TensorLayer">
    <img src="https://www.tensorflow.org/images/tf_logo_transp.png" style=": right; margin-left: 5px; margin-bottom: 5px;">
    </div>
</div>
-->
<a href="http://github.com/zsdonghao/tensorlayer">
<div align="center">
	<img src="img/img_tensorlayer.png" width="30%" height="30%"/>
</div>
</a>

[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/tensorlayer/Lobby#?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)
[![Help Wanted Issues](https://badge.waffle.io/zsdonghao/tensorlayer.svg?label=up-for-grabs&title=Help Wanted Issues)](https://waffle.io/zsdonghao/tensorlayer)

TensorLayer æ˜¯åŸºäº [Google TensorFlow](https://www.tensorflow.org) å¼€å‘çš„æ·±åº¦å­¦ä¹ ä¸å¢å¼ºå­¦ä¹ åº“ã€‚å®ƒæä¾›ä¸»æµçš„æ·±åº¦å­¦ä¹ ä¸å¢å¼ºå­¦ä¹ æ¨¡å—ï¼Œå¯ä»¥éå¸¸å®¹æ˜“åœ°è‡ªå®šä¹‰æ¨¡å‹ä»¥è§£å†³äººå·¥æ™ºèƒ½é—®é¢˜ã€‚

TensorLayer grow out from a need to combine the power of TensorFlow with the right building modules for deep neural networks. According to our years of research and practical experiences of tackling real-world machine learning problems, we come up with three design goals for TensorLayer:

- **Simplicity**: we make TensorLayer easy to work with by providing mass tutorials that can be deployed and run through in minutes. A TensorFlow user may find it easier to bootstrap with the simple, high-level APIs provided by TensorLayer, and then deep dive into their implementation details if need. 
- **Flexibility**: developing an effective DL algorithm for a specific domain typically requires careful tunings from many aspects. Without the loss of simplicity, TensorLayer allows users to customize their modules by manipulating the native APIs of TensorFlow (e.g., training parameters, iteration control and tensor components).
- **Performance**: TensorLayer aims to provide zero-cost abstraction for TensorFlow. With its first-class support for TensorFlow, it can easily run on either heterogeneous platforms or multiple computation nodes without compromise in performance.

å…³äº TensorLayer ä¸€ä¸ªæœ€å¸¸è§çš„é—®é¢˜å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å¼€å‘è¿™ä¸ªåº“ï¼Œä¸å…¶ä»–åº“å¦‚ [Keras](https://github.com/fchollet/keras) å’Œ [Tflearn](https://github.com/tflearn/tflearn)æœ‰ä»€ä¹ˆåŒºåˆ«ã€‚
TensorLayer å’Œè¿™äº›åº“æœ€å¤§çš„åŒºåˆ«åœ¨äºçµæ´»æ€§å’Œè¿è¡Œé€Ÿåº¦ã€‚æ·±åº¦å­¦ä¹ ç”¨æˆ·ä¼šå‘ç°ä½¿ç”¨ Keras å’Œ Tflearn èƒ½å¤Ÿéå¸¸å¿«çš„ä¸Šæ‰‹ï¼ˆå½“ç„¶ TensorLayer ä¹Ÿæä¾›ä¸å®ƒä»¬ç±»ä¼¼çš„ç®€å• APIsï¼‰ï¼Œè¿™äº›åº“æä¾›é«˜å±‚æŠ½è±¡çš„APIï¼Œå¯¹å¼€å‘è€…éšè—äº†æ·±åº¦å­¦ä¹ å¼•æ“çš„ç»†èŠ‚ã€‚è¿™ä¼šè®©ç”¨æˆ·å¾ˆéš¾ä»åº•å±‚ä¸­ä¿®æ”¹å’Œä¼˜åŒ–ï¼Œè€Œè¿™å¾€å¾€åœ¨ç‰¹å®šé¢†åŸŸæ—¶éœ€è¦è€ƒè™‘çš„ã€‚å°½ç®¡å¦‚æ­¤ï¼Œçµæ´»æ€§ä¸ä¼šå¯¼è‡´æ•ˆç‡çš„é™ä½ï¼ŒTensorLayer å¯ä»¥åˆ†å¸ƒå¼å’Œå¤šæ ·åŒ–éƒ¨ç½²ä»¥æœ€ä¼˜åŒ–è¿è¡Œé€Ÿåº¦ã€‚æ­¤å¤–ï¼ŒTensorLayer è¿˜èƒ½å’Œå¾ˆå¤šåº“æ— ç¼ä½¿ç”¨ï¼Œå¦‚ [TF-Slim](https://github.com/tensorflow/models/tree/master/slim) ç­‰ç­‰ã€‚

# è¯‘è€…æ³¨

ç®€å•æ¥è®² TensorLayer æ˜¯ä¸€ä¸ªé€‚ç”¨äºä¸åŒæ°´å¹³ç”¨æˆ·ä½¿ç”¨çš„åº“ã€‚å¯¹äºåˆå­¦è€…ï¼ŒTensorLayer æä¾›å¤§é‡ç®€å•çš„APIå’Œå¤§é‡çš„æ•™ç¨‹ï¼›å¯¹äºä¸­çº§ç”¨æˆ·ï¼ŒTensorLayer çš„çµæ´»æ€§å’Œé€æ˜æ€§ä¼˜åŠ¿èƒ½å¤§å¤§ä½“ç°å‡ºæ¥ï¼ˆV1.2ç‰ˆæœ¬æ˜¯å¾ˆå¥½çš„ä¾‹å­ï¼‰ï¼›å¯¹äºé«˜çº§ç”¨æˆ·ï¼Œè¿è¡Œé€Ÿåº¦å¿«å’Œè·¨å¹³å°ä¼˜åŠ¿ä¼šä½“ç°å‡ºæ¥ã€‚è¿™æ ·çš„å¥½å¤„æ˜¯ä½œä¸ºç”¨æˆ·ï¼Œæˆ‘ä»¬ä¸éœ€è¦å› ä¸ºåœ¨ä¸åŒçš„å­¦ä¹ é˜¶æ®µï¼Œè€Œå»å­¦ä¸åŒçš„åº“äº†ã€‚

ğŸŒğŸŒğŸŒ æˆ‘ä»¬å»ºè®®ä½ åœ¨[Github](http://github.com/zsdonghao/tensorlayer) ä¸Šstarå’Œwatch[å®˜æ–¹é¡¹ç›®](http://github.com/zsdonghao/tensorlayer)ï¼Œè¿™æ ·å½“å®˜æ–¹æœ‰æ›´æ–°æ—¶ï¼Œä½ ä¼šç«‹å³çŸ¥é“ã€‚æœ¬æ–‡æ¡£ä¸º[å®˜æ–¹RTDæ–‡æ¡£](https://github.com/zsdonghao/tensorlayer)çš„ç¿»è¯‘ç‰ˆï¼Œæ›´æ–°é€Ÿåº¦ä¼šæ¯”è‹±æ–‡åŸç‰ˆæ…¢ï¼Œè‹¥ä½ çš„è‹±æ–‡è¿˜è¡Œï¼Œæˆ‘ä»¬å»ºè®®ä½ ç›´æ¥é˜…è¯»[å®˜æ–¹RTDæ–‡æ¡£](https://github.com/zsdonghao/tensorlayer)

â¤ï¸â¤ï¸â¤ï¸ TensorLayeré¦–æ‰¹å¼€å‘è€…åŒ…æ‹¬ä¸­å›½äººï¼Œæˆ‘ä»¬æ‰¿è¯ºå°†ä¸€ç›´æ”¯æŒä¸­å›½ç¤¾åŒº


# å®‰è£…

TensorLayer è¿è¡Œéœ€è¦ TensorFlow, numpy å’Œ matplotlibã€‚ å¯¹äº GPU åŠ é€Ÿï¼Œéœ€è¦å®‰è£… CUDA å’Œ cuDNNã€‚è¯·åœ¨ [è¿™é‡Œ](http://tensorlayercn.readthedocs.io/zh/latest/user/installation.html) æŸ¥çœ‹æ›´å¤šå®‰è£…ç»†èŠ‚ã€‚

å¦‚æœæ‚¨å·²ç»å®‰è£…è¿‡ TensorFlowï¼Œæœ€ç®€å•çš„å®‰è£…å‘½ä»¤å¦‚ä¸‹ (ä»¥è‹±æ–‡[Github](http://github.com/zsdonghao/tensorlayer)ä¸ºå‡†)ï¼š

```bash
[for stable version] pip install tensorlayer==1.2.2b    
[for master version] pip install git+https://github.com/zsdonghao/tensorlayer.git
```

# æ‚¨ç¬¬ä¸€ä¸ªç¨‹åº

ç¬¬ä¸€ä¸ªç¨‹åºè®­ç»ƒä¸€ä¸ªå¤šå±‚ç¥ç»ç½‘ç»œæ¥å¯¹ MNIST é˜¿æ‹‰ä¼¯æ•°å­—å›¾ç‰‡é›†è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ç®€å•äº† [scikit](http://scikit-learn.org/stable/)å¼å‡½æ•°ï¼Œå¦‚ ``fit()`` å’Œ ``test()`` ã€‚ 

```python
import tensorflow as tf
import tensorlayer as tl

sess = tf.InteractiveSession()

# å‡†å¤‡æ•°æ®
X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1,784))

# å®šä¹‰ placeholder
x = tf.placeholder(tf.float32, shape=[None, 784], name='x')
y_ = tf.placeholder(tf.int64, shape=[None, ], name='y_')

# å®šä¹‰æ¨¡å‹
network = tl.layers.InputLayer(x, name='input_layer')
network = tl.layers.DropoutLayer(network, keep=0.8, name='drop1')
network = tl.layers.DenseLayer(network, n_units=800, act = tf.nn.relu, name='relu1')
network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2')
network = tl.layers.DenseLayer(network, n_units=800, act = tf.nn.relu, name='relu2')
network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')

# å®šä¹‰è¾“å‡ºã€æŸå¤±å‡½æ•°å’Œè¡¡é‡æŒ‡æ ‡
# tl.cost.cross_entropy åœ¨å†…éƒ¨ä½¿ç”¨ tf.nn.sparse_softmax_cross_entropy_with_logits() å®ç° softmax
network = tl.layers.DenseLayer(network, n_units=10, act = tf.identity, name='output_layer')
y = network.outputs
cost = tl.cost.cross_entropy(y, y_)
correct_prediction = tf.equal(tf.argmax(y, 1), y_)
acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
y_op = tf.argmax(tf.nn.softmax(y), 1)

# å®šä¹‰ optimizer
train_params = network.all_params
train_op = tf.train.AdamOptimizer(learning_rate=0.0001, beta1=0.9, beta2=0.999,
                            epsilon=1e-08, use_locking=False).minimize(cost, var_list=train_params)

# åˆå§‹åŒ–æ‰€æœ‰å‚æ•°
sess.run(tf.initialize_all_variables())

# åˆ—å‡ºæ¨¡å‹ä¿¡æ¯
network.print_params()
network.print_layers()

# è®­ç»ƒæ¨¡å‹, ä½†å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå®ç°ç»†èŠ‚ï¼Œæˆ–æƒ³æˆä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸçš„ä¸“å®¶ï¼Œ
# æˆ‘ä»¬é¼“åŠ±ä½¿ç”¨ tl.iterate.minibatches() æ¥è®­ç»ƒæ¨¡å‹
tl.utils.fit(sess, network, train_op, cost, X_train, y_train, x, y_,
            acc=acc, batch_size=500, n_epoch=500, print_freq=5,
            X_val=X_val, y_val=y_val, eval_train=False)

# è¯„ä¼°æ¨¡å‹
tl.utils.test(sess, network, acc, X_test, y_test, x, y_, batch_size=None, cost=cost)

# æŠŠæ¨¡å‹ä¿å­˜æˆ .npz æ–‡ä»¶
tl.files.save_npz(network.all_params , name='model.npz')

sess.close()
```

æˆ‘ä»¬æä¾›ç®€å•çš„ APIs å¦‚ `fit()` , `test()`ï¼Œè¿™å’Œ Scikit-learn, Keras å¾ˆç›¸è¯†ï¼Œéƒ½æ˜¯ä¸ºäº†åŠ å¿«ç¼–ç¨‹é€Ÿåº¦ã€‚ä¸è¿‡ï¼Œå¦‚æœæ‚¨æƒ³æ›´å¥½åœ°æ§åˆ¶è®­ç»ƒè¿‡ç¨‹ï¼Œæ‚¨å¯ä»¥åœ¨æ‚¨çš„ä»£ç ä¸­ä½¿ç”¨ TensorFlow åŸæœ¬çš„æ–¹æ³•ï¼Œå¦‚ `sess.run` ï¼ˆ`tutorial_mnist.py` æä¾›äº†ä¸€äº›ä¾‹å­ï¼‰ã€‚æ›´å¤šå…³äºDLå’ŒRLçš„ä¾‹å­ï¼Œè¯·è§ [è¿™é‡Œ](http://tensorlayer.readthedocs.io/en/latest/user/example.html)ã€‚

# æ–‡æ¡£

æ–‡æ¡£  [[CN]](http://tensorlayercn.readthedocs.io) [[EN]](http://tensorlayer.readthedocs.io) [[PDF]](https://media.readthedocs.org/pdf/tensorlayer/latest/tensorlayer.pdf) [[Epub]](http://readthedocs.org/projects/tensorlayer/downloads/epub/latest/) [[HTML]](http://readthedocs.org/projects/tensorlayer/downloads/htmlzip/latest/) ä¸ä»…æè¿°äº†å¦‚ä½•ä½¿ç”¨ TensorLayer APIsï¼Œå®ƒè¿˜æä¾›äº†å¤§é‡çš„æ·±åº¦å­¦ä¹ æ•™ç¨‹åŒ…æ‹¬ä¸åŒç±»å‹çš„ç¥ç»ç½‘ç»œã€å¢å¼ºå­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ç­‰ã€‚

æˆ‘ä»¬è¿˜æä¾›äº† Google TensorFlow ç½‘ç«™ä¾‹å­çš„æ¨¡å—åŒ–å®ç°ï¼Œå› æ­¤æ‚¨è¿˜å¯ä»¥åŒæ—¶é˜…è¯» TensorFlow çš„ä¾‹å­æ•™ç¨‹ [[en]](https://www.tensorflow.org/versions/master/tutorials/index.html) [[cn]](http://wiki.jikexueyuan.com/project/tensorflow-zh/) ã€‚



# è´¡çŒ®æŒ‡å—

<!--
TensorLayer èµ·åˆæ˜¯å¸å›½ç†å·¥å¤§å­¦çš„å†…éƒ¨é¡¹ç›®ï¼Œç”¨æ¥å¸®åŠ©ç ”ç©¶äººå‘˜éªŒè¯æ–°çš„ç®—æ³•ã€‚ç°åœ¨å®ƒé¼“åŠ±å…¨ä¸–ç•Œçš„äººå·¥æ™ºèƒ½çˆ±å¥½è€…ä»¬å‚ä¸å¼€å‘ï¼Œä»¥ä¿ƒè¿›å­¦æœ¯å’Œåº”ç”¨äº¤æµã€‚æ‚¨å¯ä»¥å’Œæˆ‘ä»¬è”ç³»è®¨è®ºæ‚¨çš„æƒ³æ³•ï¼Œæˆ–è€…åœ¨å®˜æ–¹ Github ä¸Šå‘èµ· Fork ä¸ Pull è¯·æ±‚ã€‚
 -->
TensorLayer is a major ongoing research project in Data Science Institute, Imperial College London.
TensorLayer contributors are from Imperial College, Tsinghua University, Carnegie Mellon University, Google, Microsoft, Bloomberg and etc.
The goal of the project is to develop a compositional language while complex learning systems
can be build through composition of neural network modules.
The whole development is now participated by numerous contributors [here](https://github.com/zsdonghao/tensorlayer/releases).


- ğŸ‡¬ğŸ‡§ If you are in London, we can discuss in person. Drop us an email to organize a meetup: tensorlayer@gmail.com.
- ğŸ‡¨ğŸ‡³ æˆ‘ä»¬æœ‰å®˜æ–¹çš„ [ä¸­æ–‡æ–‡æ¡£](http://tensorlayercn.readthedocs.io/zh/latest)ã€‚å¦å¤–, æˆ‘ä»¬å»ºç«‹äº†å¤šç§äº¤æµæ¸ é“ï¼Œå¦‚[QQ ç¾¤](img/img_qq.png)å’Œå¾®ä¿¡ç¾¤*ï¼ˆç”³è¯·å…¥ç¾¤æ—¶è¯·starè¯¥é¡¹ç›®ï¼Œå¹¶å‘ŠçŸ¥githubç”¨æˆ·åï¼‰*. éœ€åŠ å…¥å¾®ä¿¡ç¾¤ï¼Œè¯·å°†ä¸ªäººä»‹ç»å’Œå¾®ä¿¡å·å‘é€åˆ° tensorlayer@gmail.com.
- ğŸ‡¹ğŸ‡­ à¹€à¸£à¸²à¸‚à¸­à¹€à¸£à¸µà¸¢à¸™à¹€à¸Šà¸´à¸à¸™à¸±à¸à¸à¸±à¸’à¸™à¸²à¸„à¸™à¹„à¸—à¸¢à¸—à¸¸à¸à¸„à¸™à¸—à¸µà¹ˆà¸ªà¸™à¹ƒà¸ˆà¸ˆà¸°à¹€à¸‚à¹‰à¸²à¸£à¹ˆà¸§à¸¡à¸—à¸µà¸¡à¸à¸±à¸’à¸™à¸² TensorLayer à¸•à¸´à¸”à¸•à¹ˆà¸­à¸ªà¸­à¸šà¸–à¸²à¸¡à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡à¹„à¸”à¹‰à¸—à¸µà¹ˆ tensorlayer@gmail.com.

# ç‰ˆæƒ

TensorLayer is releazed under the Apache 2.0 license.

